{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ulysses_remake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehanbchinoy/Math-156-project/blob/main/ulysses_remake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i3v2jf9YcpF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import base64\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NFP7eU_glCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fa5e09-6c41-4fae-9e26-2023d90eee28"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN6_apUZYcpN"
      },
      "source": [
        "#read the txt file to str\n",
        "master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/ulysses_james_joyce.txt\"\n",
        "req = requests.get(master)\n",
        "ulysses = req.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-pvavTZZO0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8572ef1e-fc27-4ff0-c14d-64099aa76059"
      },
      "source": [
        "len(ulysses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1552235"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5gsSW7IYcpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31edfe81-1dea-4f2e-cbb7-ddf6e7e64e3b"
      },
      "source": [
        "#unique characters in the file\n",
        "vocab = sorted(set(ulysses))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJ7EUG0YcpS"
      },
      "source": [
        "#all the characters appeared in Ulysses\n",
        "#vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0f9GH9LYcpT"
      },
      "source": [
        "## Process the Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdR5PlV7YcpW"
      },
      "source": [
        "### Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOilAdPLYcpW"
      },
      "source": [
        "#function that converts character tokens to numeric ids\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQgNRjAYcpY"
      },
      "source": [
        "#function that converts numeric ids to character tokens\n",
        "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTVgSQ3PYcpZ"
      },
      "source": [
        "#function that combines character tokens back to strings\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ENNjjV2Ycpa"
      },
      "source": [
        "### Creating training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIrPa7cOYcpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a88ffd-95c2-4b83-cd4e-014be323085b"
      },
      "source": [
        "#convert ulysses from strings to numeric vector\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(ulysses, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1552235,), dtype=int64, numpy=array([114,   3,  36, ...,  13,   2,   1])>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdFUYl9oYcpc"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmbgr6zoYcpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0f94e4-43b3-42cf-8e23-ae84aa4ac14e"
      },
      "source": [
        "#verify the ids dataset is correct\n",
        "for ids in ids_dataset.take(100):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "—\n",
            " \n",
            "I\n",
            " \n",
            "—\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "[\n",
            " \n",
            "1\n",
            " \n",
            "]\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "S\n",
            "t\n",
            "a\n",
            "t\n",
            "e\n",
            "l\n",
            "y\n",
            ",\n",
            " \n",
            "p\n",
            "l\n",
            "u\n",
            "m\n",
            "p\n",
            " \n",
            "B\n",
            "u\n",
            "c\n",
            "k\n",
            " \n",
            "M\n",
            "u\n",
            "l\n",
            "l\n",
            "i\n",
            "g\n",
            "a\n",
            "n\n",
            " \n",
            "c\n",
            "a\n",
            "m\n",
            "e\n",
            " \n",
            "f\n",
            "r\n",
            "o\n",
            "m\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "s\n",
            "t\n",
            "a\n",
            "i\n",
            "r\n",
            "h\n",
            "e\n",
            "a\n",
            "d\n",
            ",\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "r\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "a\n",
            " \n",
            "b\n",
            "o\n",
            "w\n",
            "l\n",
            " \n",
            "o\n",
            "f\n",
            "\r\n",
            "\n",
            "\n",
            "l\n",
            "a\n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2eM_YBfYcpf"
      },
      "source": [
        "#split the text into sequences with length 100\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(ulysses)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PYtWdNiYcph"
      },
      "source": [
        "#create the sequences based on sequences with length 101\n",
        "#(it's 101 bc we need to split it into training and target sequences)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuYNxm21Ycph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989218a4-ed2b-4c19-ae83-741d3b4934ba"
      },
      "source": [
        "#verify the sequences are created correctly\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xe2\\x80\\x94' b' ' b'I' b' ' b'\\xe2\\x80\\x94' b'\\r' b'\\n' b'\\r' b'\\n'\n",
            " b'\\r' b'\\n' b'[' b' ' b'1' b' ' b']' b'\\r' b'\\n' b'\\r' b'\\n' b'S' b't'\n",
            " b'a' b't' b'e' b'l' b'y' b',' b' ' b'p' b'l' b'u' b'm' b'p' b' ' b'B'\n",
            " b'u' b'c' b'k' b' ' b'M' b'u' b'l' b'l' b'i' b'g' b'a' b'n' b' ' b'c'\n",
            " b'a' b'm' b'e' b' ' b'f' b'r' b'o' b'm' b' ' b't' b'h' b'e' b' ' b's'\n",
            " b't' b'a' b'i' b'r' b'h' b'e' b'a' b'd' b',' b' ' b'b' b'e' b'a' b'r'\n",
            " b'i' b'n' b'g' b' ' b'a' b' ' b'b' b'o' b'w' b'l' b' ' b'o' b'f' b'\\r'\n",
            " b'\\n' b'l' b'a' b't' b'h' b'e' b'r' b' ' b'o'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpzHsJWLYcpi"
      },
      "source": [
        "#generate the target sequence for each sequence\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOt0l5gbYcpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e1da5e-362b-4e9c-c97d-5ec0b1e2fbd7"
      },
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsocR7AKYcpk"
      },
      "source": [
        "### Create Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azDocwGlYcpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd8a77b-7c42-495c-a2a5-093460966fd5"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJIrNKWAYcpl"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-c_Gyt7Ycpm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG4Iu-L_Ycpn"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        " \n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc7mIDZxYcpo"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncgwtuMYcpp"
      },
      "source": [
        "## Try The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D7dES8Ha4DP"
      },
      "source": [
        "Run the model to see if the output is as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkU2FKgRYcpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2600e96c-bc45-4122-dece-bcffdac06a4c"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 122) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-I7jtTccTAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacf82a4-3906-4330-abde-2577f7073033"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  31232     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  125050    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,094,586\n",
            "Trainable params: 4,094,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvjd5wIcWTZ"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlXS0mQScYDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7d33c7-8ffe-4411-d2cf-bcebb9feb710"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'larged glands, mumps,\\r\\nquinsy, bunions, hayfever, bedsores, ringworm, floating kidney,\\r\\nDerbyshire n'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'k&)9\\xc3\\xaerLD&,q\\xc3\\xbcEm\\xc3\\xb3\\xc4\\x81\\xc3\\xa6TZ\\xc3\\xbapP1FO\\xc3\\xb2Jr\\xe2\\x80\\xa0\\xc3\\xae9Ut&sq\\xe2\\x80\\xa2(jC\\xc3\\x80\\xc3\\xa4\\xe2\\x80\\xa2\\xe2\\x80\\xa0Q\\xe2\\x9c\\xa0fik-\\xc3\\xb42(\\xc2\\xb0i &uB2\\xc5\\xbf\\xc3\\x86\\xc4\\x81\\xc3\\x9c0\\xe2\\x80\\xa0\\xc3\\xbc\\xc3\\x865\\xc3\\x89.5\\xc3\\xbcR\\xc3\\x893fY\\xc3\\xb3\\xc3\\x9c\\r7\\xc3\\xb3SqT\\xe2\\x80\\x9d\\xc5\\x93lnQT?g\\xe2\\x80\\xa61\\xc2\\xb0\\xc4\\x81Ya'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt8Gn-Nicdc6"
      },
      "source": [
        "## Train The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-DhtPJpcntV"
      },
      "source": [
        "### Add optimizer and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPAsfw_4cl-V"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OETdXQo4cs7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9d7c8f-d14f-427e-b8bf-2ef78fc96906"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 122)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.8036766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cNBocToc4ML"
      },
      "source": [
        "Check to make sure the loss is roughly the same as the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6U2zfScuiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2458d4-617a-4c4b-9b03-b8de35e22c1a"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121.957985"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pooNw_7GcyFM"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP9HXRw2eabi"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK2tK73qeTNC"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "# Name of the checkpoint files\n",
        "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_dir,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eir0nBPdexh-"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZQxpZTSev23"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh1camR9e-d4",
        "outputId": "c886dfa3-0b09-47e7-ae71-7e9f3aa9eea9"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "240/240 [==============================] - 36s 136ms/step - loss: 2.6122\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 34s 136ms/step - loss: 2.0159\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 34s 136ms/step - loss: 1.7965\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.6618\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.5712\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.5024\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.4451\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.3930\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.3435\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.2947\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 34s 134ms/step - loss: 1.2438\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 34s 134ms/step - loss: 1.1933\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.1410\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.0893\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 1.0382\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 0.9897\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 0.9442\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 0.9033\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 0.8680\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 34s 135ms/step - loss: 0.8362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsG90V5ayWNl"
      },
      "source": [
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/checkpoint_epoch20')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEuxCLwFzsz1",
        "outputId": "71e9ce0a-cd51-4296-db22-0005d5384b84"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_epoch20')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_epoch20/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_epoch20/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f4e83a40050> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URrnEhSQnMr8"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPFDHclLnM9T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhodc1uSim_0"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJn4NnmrnSgW"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMKebVzdnVPK",
        "outputId": "055fae51-3f46-4fe3-cacc-0ee9ee7d70f1"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Duke'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duke mammad\r\n",
            "no hurry to throw for, throb in his mina border. What do they invent out\r\n",
            "too downs what means to keep them to defull to think of home shewness.\r\n",
            "\r\n",
            "—Who is he knew, Stephen interfected his armpit, Greem house. Two and\r\n",
            "pinstrite of it. It was dear gord.\r\n",
            "\r\n",
            "—He knows the speech, Mr O’Madden Burke said.\r\n",
            "\r\n",
            "—You’re not serent as hurried simply. That was her coying born for their teeth\r\n",
            "cold not her nostrils.\r\n",
            "\r\n",
            "—Both of his feet.\r\n",
            "\r\n",
            "Hoopewryt. Tommy was watching mustaritice. The Royal Davath Eggs. Waltz.\r\n",
            "Acil’s words are girls. One please. Time that could he say. She got\r\n",
            "a hackney came to Dublin. With a cough knocking over and he say for you. Call it Alexan\r\n",
            "at their eyes would tempting manking armour? Are yested on afraid of\r\n",
            "blooded mass. Tranquility beside the accumulous. Come in to them, he had\r\n",
            "outlioner you: mind behind a kind of a moment. Bowing themselves in\r\n",
            "that her church dissolse in the laugh but behind the dog after them, to set us\r\n",
            "     God’s cifference withy Henr \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.460049390792847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymt79eltFlW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}