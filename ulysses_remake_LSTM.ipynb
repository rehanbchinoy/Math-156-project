{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ulysses_remake_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehanbchinoy/Math-156-project/blob/main/ulysses_remake_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i3v2jf9YcpF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import base64\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NFP7eU_glCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fa5e09-6c41-4fae-9e26-2023d90eee28"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN6_apUZYcpN"
      },
      "source": [
        "#read the txt file to str\n",
        "master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/ulysses_james_joyce.txt\"\n",
        "req = requests.get(master)\n",
        "ulysses = req.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-pvavTZZO0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8572ef1e-fc27-4ff0-c14d-64099aa76059"
      },
      "source": [
        "len(ulysses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1552235"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5gsSW7IYcpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31edfe81-1dea-4f2e-cbb7-ddf6e7e64e3b"
      },
      "source": [
        "#unique characters in the file\n",
        "vocab = sorted(set(ulysses))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJ7EUG0YcpS"
      },
      "source": [
        "#all the characters appeared in Ulysses\n",
        "#vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0f9GH9LYcpT"
      },
      "source": [
        "## Process the Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdR5PlV7YcpW"
      },
      "source": [
        "### Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOilAdPLYcpW"
      },
      "source": [
        "#function that converts character tokens to numeric ids\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQgNRjAYcpY"
      },
      "source": [
        "#function that converts numeric ids to character tokens\n",
        "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTVgSQ3PYcpZ"
      },
      "source": [
        "#function that combines character tokens back to strings\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ENNjjV2Ycpa"
      },
      "source": [
        "### Creating training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIrPa7cOYcpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a88ffd-95c2-4b83-cd4e-014be323085b"
      },
      "source": [
        "#convert ulysses from strings to numeric vector\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(ulysses, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1552235,), dtype=int64, numpy=array([114,   3,  36, ...,  13,   2,   1])>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdFUYl9oYcpc"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmbgr6zoYcpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0f94e4-43b3-42cf-8e23-ae84aa4ac14e"
      },
      "source": [
        "#verify the ids dataset is correct\n",
        "for ids in ids_dataset.take(100):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "—\n",
            " \n",
            "I\n",
            " \n",
            "—\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "[\n",
            " \n",
            "1\n",
            " \n",
            "]\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "S\n",
            "t\n",
            "a\n",
            "t\n",
            "e\n",
            "l\n",
            "y\n",
            ",\n",
            " \n",
            "p\n",
            "l\n",
            "u\n",
            "m\n",
            "p\n",
            " \n",
            "B\n",
            "u\n",
            "c\n",
            "k\n",
            " \n",
            "M\n",
            "u\n",
            "l\n",
            "l\n",
            "i\n",
            "g\n",
            "a\n",
            "n\n",
            " \n",
            "c\n",
            "a\n",
            "m\n",
            "e\n",
            " \n",
            "f\n",
            "r\n",
            "o\n",
            "m\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "s\n",
            "t\n",
            "a\n",
            "i\n",
            "r\n",
            "h\n",
            "e\n",
            "a\n",
            "d\n",
            ",\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "r\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "a\n",
            " \n",
            "b\n",
            "o\n",
            "w\n",
            "l\n",
            " \n",
            "o\n",
            "f\n",
            "\r\n",
            "\n",
            "\n",
            "l\n",
            "a\n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2eM_YBfYcpf"
      },
      "source": [
        "#split the text into sequences with length 100\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(ulysses)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PYtWdNiYcph"
      },
      "source": [
        "#create the sequences based on sequences with length 101\n",
        "#(it's 101 bc we need to split it into training and target sequences)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuYNxm21Ycph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989218a4-ed2b-4c19-ae83-741d3b4934ba"
      },
      "source": [
        "#verify the sequences are created correctly\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xe2\\x80\\x94' b' ' b'I' b' ' b'\\xe2\\x80\\x94' b'\\r' b'\\n' b'\\r' b'\\n'\n",
            " b'\\r' b'\\n' b'[' b' ' b'1' b' ' b']' b'\\r' b'\\n' b'\\r' b'\\n' b'S' b't'\n",
            " b'a' b't' b'e' b'l' b'y' b',' b' ' b'p' b'l' b'u' b'm' b'p' b' ' b'B'\n",
            " b'u' b'c' b'k' b' ' b'M' b'u' b'l' b'l' b'i' b'g' b'a' b'n' b' ' b'c'\n",
            " b'a' b'm' b'e' b' ' b'f' b'r' b'o' b'm' b' ' b't' b'h' b'e' b' ' b's'\n",
            " b't' b'a' b'i' b'r' b'h' b'e' b'a' b'd' b',' b' ' b'b' b'e' b'a' b'r'\n",
            " b'i' b'n' b'g' b' ' b'a' b' ' b'b' b'o' b'w' b'l' b' ' b'o' b'f' b'\\r'\n",
            " b'\\n' b'l' b'a' b't' b'h' b'e' b'r' b' ' b'o'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpzHsJWLYcpi"
      },
      "source": [
        "#generate the target sequence for each sequence\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOt0l5gbYcpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e1da5e-362b-4e9c-c97d-5ec0b1e2fbd7"
      },
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsocR7AKYcpk"
      },
      "source": [
        "### Create Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azDocwGlYcpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd8a77b-7c42-495c-a2a5-093460966fd5"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJIrNKWAYcpl"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-c_Gyt7Ycpm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG4Iu-L_Ycpn"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        " \n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    # x, states = self.lstm(x, initial_state=states, training=training)\n",
        "    x, final_memory_state, final_carry_state =  self.lstm(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, final_memory_state, final_carry_state\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc7mIDZxYcpo"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncgwtuMYcpp"
      },
      "source": [
        "## Try The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D7dES8Ha4DP"
      },
      "source": [
        "Run the model to see if the output is as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkU2FKgRYcpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057c7187-0589-40a0-f430-87282562fa9a"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 122) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-I7jtTccTAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d1cf70-0a76-4f30-8b07-5876149d5b73"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     multiple                  31232     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " dense_7 (Dense)             multiple                  125050    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,403,258\n",
            "Trainable params: 5,403,258\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIvjd5wIcWTZ"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlXS0mQScYDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94466699-08e2-4999-c87e-298f515e9748"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'sake! Mr Dedalus exclaimed in fright. Is he dead?\\r\\n\\r\\n\\xe2\\x80\\x94Dead! Martin Cunningham cried. Not he! A boatm'\n",
            "\n",
            "Next Char Predictions:\n",
            " b' e\\xc3\\xb3)y\\xc5\\xbfs\\xc3\\xaako\\xc3\\xaeQHaZ\\xc2\\xb0!K:\\xc3\\xa6nWHJ\\xc5\\x934n;gO%\\xe2\\x9c\\xa0\\xc3\\x87Z\\xc3\\x87\\nkN5\\xc4\\x81M\\xc3\\xa8Q.%F6w/1QV79i[UNK]V?V8bO0BI\\xe2\\x80\\x9c\\xc3\\xb6\\xc3\\xa9\\xc3\\x9c-[1\\xc5\\xbfvR1i\\xc3\\xba\\xc5\\x93\\xc3\\xb9SN\\xc5\\x93+;Z\\xc3\\xaaI\\xc3\\x87F2\\xc3\\x86AYkehb&\\xc3\\xa7'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt8Gn-Nicdc6"
      },
      "source": [
        "## Train The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-DhtPJpcntV"
      },
      "source": [
        "### Add optimizer and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPAsfw_4cl-V"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OETdXQo4cs7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd949d7a-1fa2-4471-a27b-b53eb2864ead"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 122)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.803924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cNBocToc4ML"
      },
      "source": [
        "Check to make sure the loss is roughly the same as the vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6U2zfScuiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e692e4b2-ba14-4c8e-e27a-3b85a5bca5a3"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121.988174"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pooNw_7GcyFM"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP9HXRw2eabi"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK2tK73qeTNC"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "# Name of the checkpoint files\n",
        "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_dir,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eir0nBPdexh-"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZQxpZTSev23"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh1camR9e-d4",
        "outputId": "d35c7ea8-6958-488b-fdd1-7d95b6e27912"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "240/240 [==============================] - 42s 162ms/step - loss: 2.7008\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 2.1437\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.9492\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 41s 162ms/step - loss: 1.8239\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.7342\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.6673\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 41s 162ms/step - loss: 1.6154\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.5735\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.5380\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.5056\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.4775\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.4510\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.4263\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.4011\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.3776\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 40s 161ms/step - loss: 1.3539\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.3308\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 41s 162ms/step - loss: 1.3079\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 40s 162ms/step - loss: 1.2835\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 41s 162ms/step - loss: 1.2594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsG90V5ayWNl"
      },
      "source": [
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/checkpoint_LSTM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEuxCLwFzsz1",
        "outputId": "46ff1e6d-34fb-4cee-8ae9-1ade07f1cb16"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_LSTM')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_LSTM/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/ulysses_saved_model_LSTM/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff3b6fcb890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URrnEhSQnMr8"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPFDHclLnM9T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhodc1uSim_0"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    # predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "    #                                       return_state=True)\n",
        "    \n",
        "    predicted_logits, final_memory_state, final_carry_state = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    # return predicted_chars, states\n",
        "    return predicted_chars, final_memory_state, final_carry_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJn4NnmrnSgW"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMKebVzdnVPK",
        "outputId": "3e59d02f-a0c3-47ad-a49a-f77c8444a893"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Duke '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  # next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  next_char, final_memory_state, final_carry_state = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duke f I h Hevee inimoflingofuss ushe cheshe f cse Du\r\n",
            "Blyom BLoflay the atrnths alle, hed were, pe. n w Sthensheng r he rs By ngrk I ody t ok thean m,, of estate.\r\n",
            "melug ang ile lly Loun verensis than ir Tofrinsm ad tie ain’s, ee apey Owspoun’ssth BLond. hig hof be ta slllonge allds: fay aly war fefleat haly\r\n",
            "\r\n",
            "othorarin hecately conthremesilo chansprly se Ce semasan or vysomatst h tat Lommed ad w thedlourleed cond plart, todaf sap:\r\n",
            "Maplldemis owansh thengon. anesid d\r\n",
            "\r\n",
            "e n fan in.\r\n",
            "\r\n",
            " oise bltesin, Stowhas s sesSouayps a. qus a waled\r\n",
            "thede\r\n",
            "—]\r\n",
            "lé St at NO\r\n",
            "ws_Waring.\r\n",
            "henthoond oksif the d m f y s Ahearanelblfocuthar thaivoositord s\r\n",
            "A pad aver d.\r\n",
            "jof\r\n",
            "t s\r\n",
            "mbant O.\r\n",
            "tuenganoourr byen Blominsll tty\r\n",
            "mpo bins, hechem, Tra me\r\n",
            " n aitent: De ay\r\n",
            "ulpig n flulaning ailll,\r\n",
            "ate pr Thor wivemomed  ore shered Awseiranthen wans Cou ose(Hece wo d tst.\r\n",
            "BLurs horio ong den pudang ks t aksindurrsoute\r\n",
            "ckeday d ofa\r\n",
            "folontorlend cas, athe fa hin ollag ailatrg THe ct  Wheran coust tefff thof as I  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.697915315628052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymt79eltFlW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}