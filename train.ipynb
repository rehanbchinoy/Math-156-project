{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train.ipynb","provenance":[{"file_id":"https://github.com/rehanbchinoy/Math-156-project/blob/main/dubliners_remake.ipynb","timestamp":1641524560541}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"id":"4i3v2jf9YcpF"},"source":["import tensorflow as tf\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","import numpy as np\n","import os\n","import time\n","\n","import base64\n","import requests"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NFP7eU_glCO"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VN6_apUZYcpN"},"source":["#read the txt file to str\n","master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/datasets/dubliners_james_joyce.txt\" # DUBLINERS\n","# master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/datasets/odyssey.txt\" # ODYSSEY \n","# master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/datasets/iliad.txt\" # ILIAD\n","# master = \"https://raw.githubusercontent.com/rehanbchinoy/Math-156-project/main/datasets/ulysses_james_joyce.txt\" #ULYSSES\n","req = requests.get(master)\n","dataset = req.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5gsSW7IYcpQ"},"source":["#unique characters in the file\n","vocab = sorted(set(dataset))\n","print(f'{len(vocab)} unique characters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0f9GH9LYcpT"},"source":["## Process the Text"]},{"cell_type":"markdown","metadata":{"id":"rdR5PlV7YcpW"},"source":["### Vectorize"]},{"cell_type":"code","metadata":{"id":"eOilAdPLYcpW"},"source":["#function that converts character tokens to numeric ids\n","ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPQgNRjAYcpY"},"source":["#function that converts numeric ids to character tokens\n","chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTVgSQ3PYcpZ"},"source":["#function that combines character tokens back to strings\n","def text_from_ids(ids):\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ENNjjV2Ycpa"},"source":["### Creating training examples and targets"]},{"cell_type":"code","metadata":{"id":"MIrPa7cOYcpb"},"source":["#convert dataset from strings to numeric vector\n","all_ids = ids_from_chars(tf.strings.unicode_split(dataset, 'UTF-8'))\n","all_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdFUYl9oYcpc"},"source":["ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmbgr6zoYcpe"},"source":["#verify the ids dataset is correct\n","for ids in ids_dataset.take(100):\n","    print(chars_from_ids(ids).numpy().decode('utf-8'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2eM_YBfYcpf"},"source":["#split the text into sequences with length 100\n","seq_length = 100\n","examples_per_epoch = len(dataset)//(seq_length+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PYtWdNiYcph"},"source":["#create the sequences based on sequences with length 101\n","#(it's 101 bc we need to split it into training and target sequences)\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuYNxm21Ycph"},"source":["#verify the sequences are created correctly\n","for seq in sequences.take(1):\n","  print(chars_from_ids(seq))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpzHsJWLYcpi"},"source":["#generate the target sequence for each sequence\n","def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOt0l5gbYcpj"},"source":["dataset = sequences.map(split_input_target)\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JsocR7AKYcpk"},"source":["### Create Training Batches"]},{"cell_type":"code","metadata":{"id":"azDocwGlYcpk"},"source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJIrNKWAYcpl"},"source":["## Build The Model"]},{"cell_type":"code","metadata":{"id":"x-c_Gyt7Ycpm"},"source":["# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG4Iu-L_Ycpn"},"source":["class GRU_Model(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n"," \n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTM_Model(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm = tf.keras.layers.LSTM(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n"," \n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.lstm.get_initial_state(x)\n","    # x, states = self.lstm(x, initial_state=states, training=training)\n","    x, final_memory_state, final_carry_state =  self.lstm(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, final_memory_state, final_carry_state\n","    else:\n","      return x"],"metadata":{"id":"gjx45ueleGjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xc7mIDZxYcpo"},"source":["# Choose GRU_Model or LSTM_Model\n","model = GRU_Model(\n","    # Be sure the vocabulary size matches the `StringLookup` layers.\n","    vocab_size=len(ids_from_chars.get_vocabulary()),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ncgwtuMYcpp"},"source":["## Try The Model"]},{"cell_type":"markdown","metadata":{"id":"0D7dES8Ha4DP"},"source":["Run the model to see if the output is as expected"]},{"cell_type":"code","metadata":{"id":"rkU2FKgRYcpp"},"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-I7jtTccTAI"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIvjd5wIcWTZ"},"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlXS0mQScYDG"},"source":["print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n","print()\n","print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mt8Gn-Nicdc6"},"source":["## Train The Model"]},{"cell_type":"markdown","metadata":{"id":"W-DhtPJpcntV"},"source":["### Add optimizer and loss"]},{"cell_type":"code","metadata":{"id":"rPAsfw_4cl-V"},"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OETdXQo4cs7p"},"source":["example_batch_loss = loss(target_example_batch, example_batch_predictions)\n","mean_loss = example_batch_loss.numpy().mean()\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"Mean loss:        \", mean_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls6U2zfScuiZ"},"source":["tf.exp(mean_loss).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pooNw_7GcyFM"},"source":["model.compile(optimizer='adam', loss=loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zP9HXRw2eabi"},"source":["### Configure checkpoints"]},{"cell_type":"code","metadata":{"id":"hK2tK73qeTNC"},"source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks'\n","# Name of the checkpoint files\n","#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_dir,\n","    save_weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eir0nBPdexh-"},"source":["### Execute the training"]},{"cell_type":"code","metadata":{"id":"DZQxpZTSev23"},"source":["EPOCHS = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rh1camR9e-d4"},"source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsG90V5ayWNl"},"source":["#model.save_weights('/content/drive/MyDrive/Colab Notebooks/checkpoint')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEuxCLwFzsz1"},"source":["model.save('/content/drive/MyDrive/Colab Notebooks/saved_model') # RENAME FOR CLARITY"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URrnEhSQnMr8"},"source":["## Generate Text"]},{"cell_type":"markdown","metadata":{"id":"cPFDHclLnM9T"},"source":[""]},{"cell_type":"code","metadata":{"id":"bhodc1uSim_0"},"source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","\n","    # Return the characters and model state.\n","    return predicted_chars, states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJn4NnmrnSgW"},"source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMKebVzdnVPK"},"source":["start = time.time()\n","states = None\n","next_char = tf.constant(['Duke'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"execution_count":null,"outputs":[]}]}